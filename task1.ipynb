{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check for common deployment errors first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This should show three nodes with 500 - 1000 GB each (951 in this case). Ambari -> 'Start All' should be done first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFS Remaining: 2934991313408 (2.67 TB)\n",
      "DFS Remaining: 999476915712 (930.84 GB)\n",
      "DFS Remaining%: 93.06%\n",
      "DFS Remaining: 1001890696704 (933.08 GB)\n",
      "DFS Remaining%: 93.28%\n",
      "DFS Remaining: 933623700992 (869.50 GB)\n",
      "DFS Remaining%: 86.93%\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfsadmin -report | grep \"DFS Remaining\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Create Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import spark_setup\n",
    "spark_setup.setup_pyspark_env()\n",
    "import spark_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambari - http://10.0.1.21:8080\n",
      "All Applications - http://10.0.1.23:8088/cluster\n",
      "CPU times: user 16 ms, sys: 20 ms, total: 36 ms\n",
      "Wall time: 19.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sc = spark_utils.get_spark_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ss = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "hdfs_client = InsecureClient(\"http://cluster1:50070\", user='hdfs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Download task data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# place yours here\n",
    "student_id = 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your region is: southcentralus\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "student_region = json.loads(open(\"../azure/regions.json\").read())[\"student{}\".format(student_id)]\n",
    "print \"your region is:\", student_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://lsml1southcentralus.blob.core.windows.net/data/task1.zip\n"
     ]
    }
   ],
   "source": [
    "task_data_link = \"https://lsml1{}.blob.core.windows.net/data/task1.zip\".format(student_region)\n",
    "print task_data_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56 ms, sys: 20 ms, total: 76 ms\n",
      "Wall time: 5min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# you can see progress in tmux (tab with running jupyter notebook)\n",
    "import os\n",
    "os.system(\"wget {} -O /data/task1.zip\".format(task_data_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 4 ms, total: 4 ms\n",
      "Wall time: 1.06 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# unzip task1.zip (many zip files inside)\n",
    "os.system(\"unzip /data/task1.zip -d /data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136M\t/data/clicks_test.csv.zip\r\n",
      "390M\t/data/clicks_train.csv.zip\r\n",
      "33M\t/data/documents_categories.csv.zip\r\n",
      "126M\t/data/documents_entities.csv.zip\r\n",
      "16M\t/data/documents_meta.csv.zip\r\n",
      "121M\t/data/documents_topics.csv.zip\r\n",
      "478M\t/data/events.csv.zip\r\n",
      "30G\t/data/page_views.csv.zip\r\n",
      "149M\t/data/page_views_sample.csv.zip\r\n",
      "2.6M\t/data/promoted_content.csv.zip\r\n",
      "100M\t/data/sample_submission.csv.zip\r\n",
      "32G\t/data/task1.zip\r\n"
     ]
    }
   ],
   "source": [
    "# verify that you're all set\n",
    "! du -sh /data/*.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Load data to HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "https://www.kaggle.com/c/outbrain-click-prediction/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def timeit(method):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        print '%r (%r, %r) %2.2f sec' % \\\n",
    "              (method.__name__, args, kw, te-ts)\n",
    "        return result\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs_client.delete(\"/task1\", recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'unzip_to_hdfs' (('clicks_test.csv.zip',), {}) 9.06 sec\n",
      "\n",
      "'unzip_to_hdfs' (('clicks_train.csv.zip',), {}) 19.03 sec\n",
      "\n",
      "'unzip_to_hdfs' (('documents_categories.csv.zip',), {}) 3.59 sec\n",
      "\n",
      "'unzip_to_hdfs' (('documents_entities.csv.zip',), {}) 6.92 sec\n",
      "\n",
      "'unzip_to_hdfs' (('documents_meta.csv.zip',), {}) 3.01 sec\n",
      "\n",
      "'unzip_to_hdfs' (('documents_topics.csv.zip',), {}) 8.65 sec\n",
      "\n",
      "'unzip_to_hdfs' (('events.csv.zip',), {}) 21.11 sec\n",
      "\n",
      "'unzip_to_hdfs' (('page_views.csv.zip',), {}) 1321.19 sec\n",
      "\n",
      "'unzip_to_hdfs' (('page_views_sample.csv.zip',), {}) 10.30 sec\n",
      "\n",
      "'unzip_to_hdfs' (('promoted_content.csv.zip',), {}) 2.68 sec\n",
      "\n",
      "'unzip_to_hdfs' (('sample_submission.csv.zip',), {}) 7.36 sec\n",
      "CPU times: user 268 ms, sys: 96 ms, total: 364 ms\n",
      "Wall time: 23min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import subprocess\n",
    "\n",
    "@timeit\n",
    "def unzip_to_hdfs(fn):\n",
    "    fn_out = fn.replace(\".zip\", \"\")\n",
    "    print subprocess.check_output(\"unzip -p /data/{0} | hadoop fs -put - /task1/{1}\".format(fn, fn_out), shell=True)\n",
    "    \n",
    "fns = [\n",
    "    \"clicks_test.csv.zip\",\n",
    "    \"clicks_train.csv.zip\",\n",
    "    \"documents_categories.csv.zip\",\n",
    "    \"documents_entities.csv.zip\",\n",
    "    \"documents_meta.csv.zip\",\n",
    "    \"documents_topics.csv.zip\",\n",
    "    \"events.csv.zip\",\n",
    "    \"page_views.csv.zip\",\n",
    "    \"page_views_sample.csv.zip\",\n",
    "    \"promoted_content.csv.zip\",\n",
    "    \"sample_submission.csv.zip\"\n",
    "]\n",
    "\n",
    "for fn in fns:\n",
    "    unzip_to_hdfs(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483.5 M  /task1/clicks_test.csv\r\n",
      "1.4 G  /task1/clicks_train.csv\r\n",
      "112.5 M  /task1/documents_categories.csv\r\n",
      "309.1 M  /task1/documents_entities.csv\r\n",
      "85.2 M  /task1/documents_meta.csv\r\n",
      "323.7 M  /task1/documents_topics.csv\r\n",
      "1.1 G  /task1/events.csv\r\n",
      "88.4 G  /task1/page_views.csv\r\n",
      "433.3 M  /task1/page_views_sample.csv\r\n",
      "13.2 M  /task1/promoted_content.csv\r\n",
      "260.5 M  /task1/sample_submission.csv\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -du -s -h /task1/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# files are written on cluster1 node only, need to balance HDFS on cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balancer bandwidth is set to 1000000000\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfsadmin -setBalancerBandwidth 1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.44 s, sys: 2.75 s, total: 8.19 s\n",
      "Wall time: 3min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "! hdfs balancer -threshold 5 > balancer.log 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Read example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pvdf = ss.read.csv(\"/task1/page_views.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('uuid', 'string'),\n",
       " ('document_id', 'string'),\n",
       " ('timestamp', 'string'),\n",
       " ('platform', 'string'),\n",
       " ('geo_location', 'string'),\n",
       " ('traffic_source', 'string')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|          uuid|document_id|timestamp|platform|geo_location|traffic_source|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|1fd5f051fba643|        120| 31905835|       1|          RS|             2|\n",
      "|8557aa9004be3b|        120| 32053104|       1|       VN>44|             2|\n",
      "|c351b277a358f0|        120| 54013023|       1|       KR>12|             1|\n",
      "|8205775c5387f9|        120| 44196592|       1|       IN>16|             2|\n",
      "|9cb0ccd8458371|        120| 65817371|       1|   US>CA>807|             2|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pvdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 140 ms, sys: 40 ms, total: 180 ms\n",
      "Wall time: 12min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2034275448"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pvdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Parquet is faster than CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "http://events.linuxfoundation.org/sites/events/files/slides/ApacheCon%20BigData%20Europe%202016%20-%20Parquet%20in%20Practice%20%26%20Detail_0.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "pvdf.write.parquet(\"/task1/page_views.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "! hadoop fs -du -s -h /task1/page_views.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pvdf2 = ss.read.parquet(\"/task1/page_views.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from IPython.display import display\n",
    "boo = pvdf2.groupBy(\"geo_location\").count().collect()\n",
    "display(boo[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "boo = pvdf.groupBy(\"geo_location\").count().collect()\n",
    "display(boo[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Convert all to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks_test.parquet done\n",
      "clicks_train.parquet done\n",
      "documents_categories.parquet done\n",
      "documents_entities.parquet done\n",
      "documents_meta.parquet done\n",
      "documents_topics.parquet done\n",
      "events.parquet done\n",
      "page_views.parquet done\n",
      "page_views_sample.parquet done\n",
      "promoted_content.parquet done\n",
      "sample_submission.parquet done\n",
      "CPU times: user 128 ms, sys: 40 ms, total: 168 ms\n",
      "Wall time: 5min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def convert_all_to_parquet():\n",
    "    task_dir = \"/task1/\"\n",
    "    all_files = hdfs_client.list(task_dir)\n",
    "    for fn in all_files:\n",
    "        if fn.endswith(\".csv\"):\n",
    "            fn_after = fn.replace(\".csv\", \".parquet\")\n",
    "            path_before = task_dir + fn\n",
    "            path_after = task_dir + fn_after\n",
    "            if fn_after not in all_files:\n",
    "                # generate parquet\n",
    "                df = ss.read.csv(path_before, header=True)\n",
    "                df.write.parquet(path_after)\n",
    "            # remove csv, we have parquet now\n",
    "            hdfs_client.delete(path_before)\n",
    "            print fn_after, \"done\"\n",
    "\n",
    "convert_all_to_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133.2 M  /task1/clicks_test.parquet\r\n",
      "367.5 M  /task1/clicks_train.parquet\r\n",
      "36.5 M  /task1/documents_categories.parquet\r\n",
      "184.0 M  /task1/documents_entities.parquet\r\n",
      "21.2 M  /task1/documents_meta.parquet\r\n",
      "183.3 M  /task1/documents_topics.parquet\r\n",
      "669.3 M  /task1/events.parquet\r\n",
      "47.3 G  /task1/page_views.parquet\r\n",
      "236.9 M  /task1/page_views_sample.parquet\r\n",
      "5.0 M  /task1/promoted_content.parquet\r\n",
      "184.2 M  /task1/sample_submission.parquet\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -du -s -h /task1/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Preview all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############### /task1/clicks_test.parquet ###############\n",
      "+----------+------+\n",
      "|display_id| ad_id|\n",
      "+----------+------+\n",
      "|  17805143|288388|\n",
      "+----------+------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/clicks_train.parquet ###############\n",
      "+----------+-----+-------+\n",
      "|display_id|ad_id|clicked|\n",
      "+----------+-----+-------+\n",
      "|         1|42337|      0|\n",
      "+----------+-----+-------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/documents_categories.parquet ###############\n",
      "+-----------+-----------+----------------+\n",
      "|document_id|category_id|confidence_level|\n",
      "+-----------+-----------+----------------+\n",
      "|    1544588|       1513|     0.263546236|\n",
      "+-----------+-----------+----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/documents_entities.parquet ###############\n",
      "+-----------+--------------------+-----------------+\n",
      "|document_id|           entity_id| confidence_level|\n",
      "+-----------+--------------------+-----------------+\n",
      "|    1539011|e01ed0c4a3e8f8f35...|0.327269624728567|\n",
      "+-----------+--------------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/documents_meta.parquet ###############\n",
      "+-----------+---------+------------+-------------------+\n",
      "|document_id|source_id|publisher_id|       publish_time|\n",
      "+-----------+---------+------------+-------------------+\n",
      "|     325048|      822|         253|2013-02-27 00:00:00|\n",
      "+-----------+---------+------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/documents_topics.parquet ###############\n",
      "+-----------+--------+------------------+\n",
      "|document_id|topic_id|  confidence_level|\n",
      "+-----------+--------+------------------+\n",
      "|     801028|     280|0.0148711250868194|\n",
      "+-----------+--------+------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/events.parquet ###############\n",
      "+----------+--------------+-----------+---------+--------+------------+\n",
      "|display_id|          uuid|document_id|timestamp|platform|geo_location|\n",
      "+----------+--------------+-----------+---------+--------+------------+\n",
      "|         1|cb8c55702adb93|     379743|       61|       3|   US>SC>519|\n",
      "+----------+--------------+-----------+---------+--------+------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/page_views.parquet ###############\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|          uuid|document_id|timestamp|platform|geo_location|traffic_source|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|68fb8eb72c49c4|    1201414| 63621328|       3|       GB>F8|             2|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/page_views_sample.parquet ###############\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|          uuid|document_id|timestamp|platform|geo_location|traffic_source|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|7504d9623fdc7e|        234| 72194818|       1|   US>CA>825|             1|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/promoted_content.parquet ###############\n",
      "+-----+-----------+-----------+-------------+\n",
      "|ad_id|document_id|campaign_id|advertiser_id|\n",
      "+-----+-----------+-----------+-------------+\n",
      "|    1|       6614|          1|            7|\n",
      "+-----+-----------+-----------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/sample_submission.parquet ###############\n",
      "+----------+--------------------+\n",
      "|display_id|               ad_id|\n",
      "+----------+--------------------+\n",
      "|  21960532|50582 190398 2293...|\n",
      "+----------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "CPU times: user 48 ms, sys: 16 ms, total: 64 ms\n",
      "Wall time: 7.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def preview_all_files():\n",
    "    task_dir = \"/task1/\"\n",
    "    all_files = hdfs_client.list(task_dir)\n",
    "    for fn in all_files:\n",
    "        df = ss.read.parquet(task_dir + fn)\n",
    "        print \"#\" * 15 + \" {0} \".format(task_dir + fn) + \"#\" * 15\n",
    "        df.show(1)\n",
    "        \n",
    "preview_all_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Register all tables to be usable in SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks_test done\n",
      "clicks_train done\n",
      "documents_categories done\n",
      "documents_entities done\n",
      "documents_meta done\n",
      "documents_topics done\n",
      "events done\n",
      "page_views done\n",
      "page_views_sample done\n",
      "promoted_content done\n",
      "sample_submission done\n",
      "CPU times: user 12 ms, sys: 28 ms, total: 40 ms\n",
      "Wall time: 2.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def register_all_tables():\n",
    "    task_dir = \"/task1/\"\n",
    "    all_files = hdfs_client.list(task_dir)\n",
    "    for fn in all_files:\n",
    "        if fn.endswith(\".parquet\"):\n",
    "            table_name = fn.replace(\".parquet\", \"\")\n",
    "            df = ss.read.parquet(task_dir + fn)\n",
    "            df.registerTempTable(table_name)\n",
    "            print table_name, \"done\"\n",
    "        \n",
    "register_all_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# SQL query example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 ms, sys: 4 ms, total: 28 ms\n",
      "Wall time: 1min 20s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(users_countb=19794967)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ss.sql(\"\"\"\n",
    "select count(distinct(uuid)) as users_countb\n",
    "from events\n",
    "\"\"\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|geo_location|\n",
      "+------------+\n",
      "|   US>MT>756|\n",
      "|       IE>16|\n",
      "|   US>MS>673|\n",
      "|       NL>10|\n",
      "|       AE>05|\n",
      "|       TH>46|\n",
      "|       IL>01|\n",
      "|          LT|\n",
      "|       MA>57|\n",
      "|       US>NY|\n",
      "|       ES>07|\n",
      "|          DZ|\n",
      "|       CO>02|\n",
      "|       CM>09|\n",
      "|       ZM>03|\n",
      "|       BG>50|\n",
      "|   US>MT>764|\n",
      "|   US>FL>548|\n",
      "|       SE>26|\n",
      "|       EC>18|\n",
      "|   US>MT>881|\n",
      "|       KZ>12|\n",
      "|       AS>00|\n",
      "|       BZ>01|\n",
      "|       HU>09|\n",
      "|       HU>18|\n",
      "|       BR>08|\n",
      "|       IN>20|\n",
      "|       CN>01|\n",
      "|       RU>01|\n",
      "+------------+\n",
      "only showing top 30 rows\n",
      "\n",
      "CPU times: user 16 ms, sys: 4 ms, total: 20 ms\n",
      "Wall time: 52.1 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# ss.sql(\"\"\"\n",
    "# select distinct(geo_location)\n",
    "# from events\n",
    "# \"\"\").show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1. Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Simple model using the following features:\n",
    "- **clicked**\n",
    "- geo_location features (country, state, dma)\n",
    "- day_of_week (from timestamp, use *date.isoweekday()*)\n",
    "- ad_id\n",
    "- ad_document_id\n",
    "- campaign_id\n",
    "- advertiser_id\n",
    "- display_document_id\n",
    "- platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Calculate features for VW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- Use DataFrame API to join tables (functions in SQL queries: https://spark.apache.org/docs/2.1.0/api/java/org/apache/spark/sql/functions.html)\n",
    "- Use Python API to calculate features and save them as text for VW (*saveAsTextFile()*)\n",
    "- Hash features in Spark (24 bits, use *sklearn.utils.murmurhash.murmurhash3_32*)\n",
    "- Split dataset in Spark into 90% train, 10% test **by display_id**, save the split for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.murmurhash import murmurhash3_32\n",
    "def hasher(x, bits):\n",
    "    return murmurhash3_32(x) % 2**bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# My code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "1. Parse geopoints (as in solution). See 'Events' on [solution](http://dsnotes.com/post/2017-01-27-lessons-learned-from-outbrain-click-prediction-kaggle-competition/) page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we start with a DataFrame\n",
    "events_df = ss.sql(\"select * from events\")\n",
    "# geopoints_parsing_df.show(3)\n",
    "\n",
    "# # we can make RDD of Rows with *.rdd\n",
    "from pyspark.sql import Row\n",
    "# events_df.rdd.take(3)\n",
    "type(events_df)\n",
    "# events_df['platform']#.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>platform</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  platform\n",
       "0        3\n",
       "1       \\N\n",
       "2        1\n",
       "3        2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.sql(\"\"\"\n",
    "select distinct(platform)\n",
    "from events\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------+\n",
      "|    col_name|data_type|comment|\n",
      "+------------+---------+-------+\n",
      "|  display_id|   string|   null|\n",
      "|        uuid|   string|   null|\n",
      "| document_id|   string|   null|\n",
      "|   timestamp|   string|   null|\n",
      "|    platform|   string|   null|\n",
      "|geo_location|   string|   null|\n",
      "+------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss.sql(\"\"\"\n",
    "describe events\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "temp = ss.sql(\"\"\"\n",
    "select distinct(geo_location)\n",
    "from events\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|geo_location|\n",
      "+------------+\n",
      "|   US>MT>756|\n",
      "|       IE>16|\n",
      "|   US>MS>673|\n",
      "|       NL>10|\n",
      "|       AE>05|\n",
      "|       TH>46|\n",
      "|       IL>01|\n",
      "|          LT|\n",
      "|       MA>57|\n",
      "|       US>NY|\n",
      "|       ES>07|\n",
      "|          DZ|\n",
      "|       CO>02|\n",
      "|       ZM>03|\n",
      "|       BG>50|\n",
      "+------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.murmurhash import murmurhash3_32\n",
    "\n",
    "def geo_to_arr(x):\n",
    "    if x.geo_location:\n",
    "        return (x.geo_location.split('>') + ['', '', ''])[:3]\n",
    "    else:\n",
    "        return ['', '', '']\n",
    "\n",
    "def handle_geo_nans(x):\n",
    "    if not x.geo_location:\n",
    "        return ''\n",
    "    return x.geo_location\n",
    "\n",
    "def handle_platform_nans(x):\n",
    "    if x['platform'] == '\\N':\n",
    "        return 1\n",
    "    return x.platform\n",
    "\n",
    "def hash_murmur(x):\n",
    "    bits = 28\n",
    "    return murmurhash3_32(x) % 2**bits\n",
    "\n",
    "def do_as_selivanov_does(x):\n",
    "    geo = geo_to_arr(x)\n",
    "    return Row(uuid=hash_murmur(x['uuid']),\n",
    "               platform=handle_platform_nans(x),\n",
    "               country=hash_murmur(geo[0]),\n",
    "               state=hash_murmur(geo[1]),\n",
    "               dma=hash_murmur(geo[2]),\n",
    "               geo_location=hash_murmur(handle_geo_nans(x)),\n",
    "               display_id=x['display_id'],\n",
    "               document_id=x['document_id'],\n",
    "               timestamp=x['timestamp'])\n",
    "\n",
    "# When it's RDD, we can use Python to create new RDD of Rows\n",
    "\n",
    "new_events_df = events_df.rdd \\\n",
    "    .map(do_as_selivanov_does)\n",
    "# new_events_df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 84 ms, sys: 24 ms, total: 108 ms\n",
      "Wall time: 6min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# much faster thanks to conversion back to DataFrame (works for simple python collections in columns)\n",
    "ss.createDataFrame(new_events_df).write.mode(\"overwrite\").parquet(\"/task1/new_events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ss.read.parquet(\"/task1/new_events\").registerTempTable(\"new_events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+-----------+------------+--------+---------+---------+---------+\n",
      "|  country|display_id|      dma|document_id|geo_location|platform|    state|timestamp|     uuid|\n",
      "+---------+----------+---------+-----------+------------+--------+---------+---------+---------+\n",
      "|175781132|         1| 70963568|     379743|   153819337|       3|193711855|       61|104110024|\n",
      "|175781132|         2|183315336|    1794259|   251276266|       2|237626922|       81| 13662515|\n",
      "|175781132|         3| 86670560|    1179111|    26577704|       2|139670629|      182|174690784|\n",
      "+---------+----------+---------+-----------+------------+--------+---------+---------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_events = ss.sql(\"select * from new_events\")\n",
    "new_events.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-------+\n",
      "|     col_name|data_type|comment|\n",
      "+-------------+---------+-------+\n",
      "|        ad_id|   string|   null|\n",
      "|  document_id|   string|   null|\n",
      "|  campaign_id|   string|   null|\n",
      "|advertiser_id|   string|   null|\n",
      "+-------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss.sql(\"\"\"\n",
    "describe promoted_content\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "events_clicks_joined = ss.sql(\n",
    "# \"\"\"\n",
    "# create TEMPORARY table test\n",
    "# USING PARQUET\n",
    "# as\"\"\"\n",
    "\"\"\"\n",
    "select \n",
    "    ne.*,\n",
    "    ct.ad_id, ct.clicked\n",
    "from \n",
    "    new_events as ne\n",
    "    join clicks_train as ct on ct.display_id = ne.display_id\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+-----------+------------+--------+---------+---------+--------+------+-------+\n",
      "|  country|display_id|      dma|document_id|geo_location|platform|    state|timestamp|    uuid| ad_id|clicked|\n",
      "+---------+----------+---------+-----------+------------+--------+---------+---------+--------+------+-------+\n",
      "|175781132|  10000108|183315336|    1126487|   251276266|       1|237626922|668707895|69455135|132815|      1|\n",
      "|175781132|  10000108|183315336|    1126487|   251276266|       1|237626922|668707895|69455135|133677|      0|\n",
      "|175781132|  10000108|183315336|    1126487|   251276266|       1|237626922|668707895|69455135|406686|      0|\n",
      "+---------+----------+---------+-----------+------------+--------+---------+---------+--------+------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events_clicks_joined.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20 ms, sys: 4 ms, total: 24 ms\n",
      "Wall time: 3min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# much faster thanks to conversion back to DataFrame (works for simple python collections in columns)\n",
    "events_clicks_joined.write.mode(\"overwrite\").parquet(\"/task1/events_clicks_joined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ss.read.parquet(\"/task1/events_clicks_joined\").registerTempTable(\"events_clicks_joined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------+\n",
      "|    col_name|data_type|comment|\n",
      "+------------+---------+-------+\n",
      "|     country|   bigint|   null|\n",
      "|  display_id|   string|   null|\n",
      "|         dma|   bigint|   null|\n",
      "| document_id|   string|   null|\n",
      "|geo_location|   bigint|   null|\n",
      "|    platform|   string|   null|\n",
      "|       state|   bigint|   null|\n",
      "|   timestamp|   string|   null|\n",
      "|        uuid|   bigint|   null|\n",
      "|       ad_id|   string|   null|\n",
      "|     clicked|   string|   null|\n",
      "+------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss.sql(\"\"\"\n",
    "describe events_clicks_joined\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-------+\n",
      "|     col_name|data_type|comment|\n",
      "+-------------+---------+-------+\n",
      "|        ad_id|   string|   null|\n",
      "|  document_id|   string|   null|\n",
      "|  campaign_id|   string|   null|\n",
      "|advertiser_id|   string|   null|\n",
      "+-------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss.sql(\"\"\"\n",
    "describe promoted_content\n",
    "\"\"\").show()\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "events_clicks_prom_joined = ss.sql(\"\"\"\n",
    "select \n",
    "    ecj.*, \n",
    "    pc.document_id as promo_document_id, pc.campaign_id, pc.advertiser_id\n",
    "from \n",
    "    events_clicks_joined as ecj\n",
    "    join promoted_content as pc on pc.ad_id = ecj.ad_id\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+-----------+------------+--------+---------+---------+---------+------+-------+-----------------+-----------+-------------+\n",
      "|  country|display_id|      dma|document_id|geo_location|platform|    state|timestamp|     uuid| ad_id|clicked|promo_document_id|campaign_id|advertiser_id|\n",
      "+---------+----------+---------+-----------+------------+--------+---------+---------+---------+------+-------+-----------------+-----------+-------------+\n",
      "|175781132|  10000022|241243482|       4099|   156524926|       1|158446504|668703976|127285375| 47454|      0|           775716|       3135|          551|\n",
      "|175781132|  10000022|241243482|       4099|   156524926|       1|158446504|668703976|127285375|120373|      1|          1041662|      10123|           58|\n",
      "|175781132|  10000084| 54356298|    2394540|   229441745|       1|184152563|668706874|222045033|161336|      0|          1402797|      20307|         2260|\n",
      "+---------+----------+---------+-----------+------------+--------+---------+---------+---------+------+-------+-----------------+-----------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events_clicks_prom_joined.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 12 ms, total: 20 ms\n",
      "Wall time: 2min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# much faster thanks to conversion back to DataFrame (works for simple python collections in columns)\n",
    "events_clicks_prom_joined.write.mode(\"overwrite\").parquet(\"/task1/events_clicks_prom_joined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ss.read.parquet(\"/task1/events_clicks_prom_joined\").registerTempTable(\"events_clicks_prom_joined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+-------+\n",
      "|         col_name|data_type|comment|\n",
      "+-----------------+---------+-------+\n",
      "|          country|   bigint|   null|\n",
      "|       display_id|   string|   null|\n",
      "|              dma|   bigint|   null|\n",
      "|      document_id|   string|   null|\n",
      "|     geo_location|   bigint|   null|\n",
      "|         platform|   string|   null|\n",
      "|            state|   bigint|   null|\n",
      "|        timestamp|   string|   null|\n",
      "|             uuid|   bigint|   null|\n",
      "|            ad_id|   string|   null|\n",
      "|          clicked|   string|   null|\n",
      "|promo_document_id|   string|   null|\n",
      "|      campaign_id|   string|   null|\n",
      "|    advertiser_id|   string|   null|\n",
      "+-----------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss.sql(\"\"\"describe events_clicks_prom_joined\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+---------+------+-------+-----------------+-----------+-------------+\n",
      "|display_id|document_id|platform|timestamp| ad_id|clicked|promo_document_id|campaign_id|advertiser_id|\n",
      "+----------+-----------+--------+---------+------+-------+-----------------+-----------+-------------+\n",
      "|  10000022|       4099|       1|668703976| 47454|      0|           775716|       3135|          551|\n",
      "|  10000022|       4099|       1|668703976|120373|      1|          1041662|      10123|           58|\n",
      "|  10000084|    2394540|       1|668706874|161336|      0|          1402797|      20307|         2260|\n",
      "+----------+-----------+--------+---------+------+-------+-----------------+-----------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss.sql(\"\"\"\n",
    "select\n",
    "    display_id, document_id, platform, timestamp, ad_id, clicked,\n",
    "    promo_document_id, campaign_id, advertiser_id\n",
    "from events_clicks_prom_joined\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.murmurhash import murmurhash3_32\n",
    "\n",
    "def h(x):\n",
    "    bits = 28\n",
    "    return murmurhash3_32(x) % 2**bits\n",
    "\n",
    "def h_with_colname(x, y):\n",
    "    return h(x) + h(y)\n",
    "\n",
    "def handle_clicked(x):\n",
    "    if x == 0:\n",
    "        return -1\n",
    "    return x\n",
    "\n",
    "def do_as_selivanov_does_2(x):\n",
    "    return Row(country=x.country+h('country'),\n",
    "               dma=x.dma+h('dma'),\n",
    "               geo_location=x.geo_location+h('geo_location'),\n",
    "               state=x.state+h('state'),\n",
    "               uuid=x.uuid+h('uuid'),\n",
    "               display_id=h_with_colname(int(x.display_id), 'display_id'),\n",
    "               display_id_no_hash=int(x.display_id),\n",
    "               document_id=h_with_colname(int(x.document_id), 'document_id'),\n",
    "               ad_id=h_with_colname(int(x.ad_id), 'ad_id'),\n",
    "               ad_id_no_hash=int(x.ad_id),\n",
    "               promo_document_id=h_with_colname(int(x.promo_document_id), 'promo_document_id'),\n",
    "               campaign_id=h_with_colname(int(x.campaign_id), 'campaign_id'),\n",
    "               advertiser_id=h_with_colname(int(x.advertiser_id), 'advertiser_id'),\n",
    "               platform=int(x.platform),\n",
    "               timestamp=int(x.timestamp),\n",
    "               clicked=handle_clicked(int(x.clicked)))\n",
    "               \n",
    "\n",
    "ecpj_hashed = events_clicks_prom_joined.rdd \\\n",
    "    .map(do_as_selivanov_does_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 84 ms, sys: 40 ms, total: 124 ms\n",
      "Wall time: 15min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# much faster thanks to conversion back to DataFrame (works for simple python collections in columns)\n",
    "ss.createDataFrame(ecpj_hashed).write.mode(\"overwrite\").parquet(\"/task1/ecpj_hashed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ss.read.parquet(\"/task1/ecpj_hashed\").registerTempTable(\"ecpj_hashed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ad_id=225494157, ad_id_no_hash=47454, advertiser_id=333740402, campaign_id=394285681, clicked=-1, country=308936875, display_id=255557651, display_id_no_hash=10000022, dma=435105654, document_id=196905228, geo_location=240564039, platform=1, promo_document_id=354778319, state=344404958, timestamp=668703976, uuid=175984321)]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# events_clicks_prom_joined.describe()\n",
    "ecpj_hashed.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ecpj_hashed.saveAsTextFile('/task1/ecpj_hashed.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def copy_text_to_local(hdfs_path, local_path):\n",
    "    if os.path.exists(local_path):\n",
    "        shutil.rmtree(local_path)\n",
    "    os.mkdir(local_path)\n",
    "    os.system('hadoop fs -copyToLocal \"{0}/*\" {1}'.format(hdfs_path, local_path))\n",
    "    os.system('cat {0}/part-* > {1}'.format(local_path, local_path + \"/merged.txt\"))\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "copy_text_to_local(\"/task1/ecpj_hashed.txt\", \"/data/ecpj_hashed.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(ad_id=248066118, ad_id_no_hash=368978, advertiser_id=329419276, campaign_id=232987658, clicked=1, country=308936875, display_id=264535752, display_id_no_hash=9999904, dma=280532732, document_id=114323303, geo_location=110616817, platform=1, promo_document_id=259186399, state=325629083, timestamp=668697477, uuid=213474776)\r\n",
      "Row(ad_id=272092809, ad_id_no_hash=428582, advertiser_id=427254936, campaign_id=395604651, clicked=-1, country=308936875, display_id=264535752, display_id_no_hash=9999904, dma=280532732, document_id=114323303, geo_location=110616817, platform=1, promo_document_id=282840021, state=325629083, timestamp=668697477, uuid=213474776)\r\n",
      "Row(ad_id=188901557, ad_id_no_hash=436596, advertiser_id=514696938, campaign_id=218286323, clicked=-1, country=308936875, display_id=264535752, display_id_no_hash=9999904, dma=280532732, document_id=114323303, geo_location=110616817, platform=1, promo_document_id=480084326, state=325629083, timestamp=668697477, uuid=213474776)\r\n",
      "Row(ad_id=203348171, ad_id_no_hash=471551, advertiser_id=325923926, campaign_id=367552217, clicked=-1, country=308936875, display_id=264535752, display_id_no_hash=9999904, dma=280532732, document_id=114323303, geo_location=110616817, platform=1, promo_document_id=430006850, state=325629083, timestamp=668697477, uuid=213474776)\r\n"
     ]
    }
   ],
   "source": [
    "!tail -n 4 /data/ecpj_hashed.txt/merged.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # !sudo pip install telepyth\n",
    "# import telepyth\n",
    "# from __future__ import print_function  \n",
    "\n",
    "# %telepyth -t 14890519403566776828\n",
    "# %telepyth 'Very magic, wow!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# from __future__ import print_function    \n",
    "\n",
    "# def txt_to_vw(in_file, out_file):\n",
    "#     with open(in_file, 'r') as in_f, open(out_file, 'w') as out_f:\n",
    "#         for l in in_f:\n",
    "#             pairs = (l.split('('))[1].split(')')[0].split(', ')\n",
    "#             d = dict(p.split('=') for p in pairs)\n",
    "#             y = d.pop('clicked')\n",
    "#             s = '{} | '.format(y) + ' '.join('{}:1'.format(v) for k,v in d.items())\n",
    "#             print(s, file=out_f)\n",
    "\n",
    "# txt_to_vw('/data/ecpj_hashed.txt/merged.txt', '/data/vw_in.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!head /data/ecpj_hashed.txt/merged.txt > /data/check.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function    \n",
    "\n",
    "def txt_to_vw(in_file, out_file, no_hash_file):\n",
    "    with open(in_file, 'r') as in_f, \\\n",
    "         open(out_file, 'w') as out_f, \\\n",
    "         open(no_hash_file, 'w') as no_hash_f:\n",
    "        for l in in_f:\n",
    "            pairs = (l.split('('))[1].split(')')[0].split(', ')\n",
    "            d = dict(p.split('=') for p in pairs)\n",
    "\n",
    "            ad_id = d.pop('ad_id_no_hash')\n",
    "            display_id = d.pop('display_id_no_hash')\n",
    "            print('{}, {}'.format(display_id, ad_id), file=no_hash_f)\n",
    "\n",
    "            y = d.pop('clicked')\n",
    "            s = '{} | '.format(y) + ' '.join('{}:1'.format(v) for k,v in d.items())\n",
    "            print(s, file=out_f)\n",
    "\n",
    "txt_to_vw('/data/ecpj_hashed.txt/merged.txt',\n",
    "          '/data/vw_in.txt',\n",
    "          '/data/no_hash_rows.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000022, 47454\r\n",
      "10000022, 120373\r\n",
      "10000084, 161336\r\n",
      "10000084, 180923\r\n",
      "10000084, 183234\r\n",
      "10000084, 211842\r\n",
      "10000084, 224008\r\n",
      "10000084, 255506\r\n",
      "10000642, 80482\r\n",
      "10000642, 117362\r\n",
      "10000642, 154918\r\n",
      "10000642, 370601\r\n",
      "10000656, 116152\r\n",
      "10000656, 173560\r\n",
      "10000656, 289967\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 15 /data/no_hash_rows.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23G /data/saved_ecpj_hashed.txt/merged.txt\r\n"
     ]
    }
   ],
   "source": [
    "# def parse_to_vw(str_in):\n",
    "!ls -sh /data/saved_ecpj_hashed.txt/merged.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13G /data/vw_in.txt\r\n"
     ]
    }
   ],
   "source": [
    "# def parse_to_vw(str_in):\n",
    "!ls -sh /data/vw_in.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Let's form test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|  col_name|data_type|comment|\n",
      "+----------+---------+-------+\n",
      "|display_id|   string|   null|\n",
      "|     ad_id|   string|   null|\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss.sql(\"\"\"describe clicks_test\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "events_clicks_joined_test = ss.sql(\n",
    "\"\"\"\n",
    "SELECT \n",
    "    ne.*,\n",
    "    ct.ad_id\n",
    "FROM \n",
    "    new_events AS ne\n",
    "    join clicks_test AS ct ON ct.display_id = ne.display_id\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----------+------------+--------+--------+---------+-------+------+\n",
      "|  country|display_id|     dma|document_id|geo_location|platform|   state|timestamp|   uuid| ad_id|\n",
      "+---------+----------+--------+-----------+------------+--------+--------+---------+-------+------+\n",
      "|175781132|  16874673|44518792|     505235|   168776120|       1|34087916|    34120|1227570| 91797|\n",
      "|175781132|  16874673|44518792|     505235|   168776120|       1|34087916|    34120|1227570|107055|\n",
      "|175781132|  16874673|44518792|     505235|   168776120|       1|34087916|    34120|1227570|158923|\n",
      "+---------+----------+--------+-----------+------------+--------+--------+---------+-------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events_clicks_joined_test.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 0 ns, total: 12 ms\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "events_clicks_joined_test.write.mode(\"overwrite\").parquet(\"/task1/events_clicks_joined_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ss.read.parquet(\"/task1/events_clicks_joined_test\").registerTempTable(\"events_clicks_joined_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------+\n",
      "|    col_name|data_type|comment|\n",
      "+------------+---------+-------+\n",
      "|     country|   bigint|   null|\n",
      "|  display_id|   string|   null|\n",
      "|         dma|   bigint|   null|\n",
      "| document_id|   string|   null|\n",
      "|geo_location|   bigint|   null|\n",
      "|    platform|   string|   null|\n",
      "|       state|   bigint|   null|\n",
      "|   timestamp|   string|   null|\n",
      "|        uuid|   bigint|   null|\n",
      "|       ad_id|   string|   null|\n",
      "+------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss.sql(\"\"\"\n",
    "describe events_clicks_joined_test\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-------+\n",
      "|     col_name|data_type|comment|\n",
      "+-------------+---------+-------+\n",
      "|        ad_id|   string|   null|\n",
      "|  document_id|   string|   null|\n",
      "|  campaign_id|   string|   null|\n",
      "|advertiser_id|   string|   null|\n",
      "+-------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ss.sql(\"\"\"\n",
    "# describe promoted_content\n",
    "# \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "events_clicks_prom_joined_test = ss.sql(\"\"\"\n",
    "select \n",
    "    ecj.*, \n",
    "    pc.document_id as promo_document_id, pc.campaign_id, pc.advertiser_id\n",
    "from \n",
    "    events_clicks_joined_test as ecj\n",
    "    join promoted_content as pc on pc.ad_id = ecj.ad_id\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+-----------+------------+--------+---------+---------+---------+------+-----------------+-----------+-------------+\n",
      "|  country|display_id|      dma|document_id|geo_location|platform|    state|timestamp|     uuid| ad_id|promo_document_id|campaign_id|advertiser_id|\n",
      "+---------+----------+---------+-----------+------------+--------+---------+---------+---------+------+-----------------+-----------+-------------+\n",
      "|175781132|  16874679|262486585|    1761355|   180291562|       2|225872597|    38382|210322864|  7343|           340745|       1444|          172|\n",
      "|175781132|  16874679|262486585|    1761355|   180291562|       2|225872597|    38382|210322864|137124|          1225364|      17621|         2988|\n",
      "|175781132|  16874679|262486585|    1761355|   180291562|       2|225872597|    38382|210322864|147706|           954430|      10363|         2603|\n",
      "+---------+----------+---------+-----------+------------+--------+---------+---------+---------+------+-----------------+-----------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events_clicks_prom_joined_test.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 4 ms, total: 8 ms\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# much faster thanks to conversion back to DataFrame (works for simple python collections in columns)\n",
    "events_clicks_prom_joined_test.write.mode(\"overwrite\").parquet(\"/task1/events_clicks_prom_joined_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ss.read.parquet(\"/task1/events_clicks_prom_joined_test\").registerTempTable(\"events_clicks_prom_joined_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+-------+\n",
      "|         col_name|data_type|comment|\n",
      "+-----------------+---------+-------+\n",
      "|          country|   bigint|   null|\n",
      "|       display_id|   string|   null|\n",
      "|              dma|   bigint|   null|\n",
      "|      document_id|   string|   null|\n",
      "|     geo_location|   bigint|   null|\n",
      "|         platform|   string|   null|\n",
      "|            state|   bigint|   null|\n",
      "|        timestamp|   string|   null|\n",
      "|             uuid|   bigint|   null|\n",
      "|            ad_id|   string|   null|\n",
      "|promo_document_id|   string|   null|\n",
      "|      campaign_id|   string|   null|\n",
      "|    advertiser_id|   string|   null|\n",
      "+-----------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss.sql(\"\"\"describe events_clicks_prom_joined_test\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+---------+------+-----------------+-----------+-------------+\n",
      "|display_id|document_id|platform|timestamp| ad_id|promo_document_id|campaign_id|advertiser_id|\n",
      "+----------+-----------+--------+---------+------+-----------------+-----------+-------------+\n",
      "|  16874679|    1761355|       2|    38382|  7343|           340745|       1444|          172|\n",
      "|  16874679|    1761355|       2|    38382|137124|          1225364|      17621|         2988|\n",
      "|  16874679|    1761355|       2|    38382|147706|           954430|      10363|         2603|\n",
      "+----------+-----------+--------+---------+------+-----------------+-----------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CLICKED!!\n",
    "ss.sql(\"\"\"\n",
    "select\n",
    "    display_id, document_id, platform, timestamp, ad_id,\n",
    "    promo_document_id, campaign_id, advertiser_id\n",
    "from events_clicks_prom_joined_test\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.murmurhash import murmurhash3_32\n",
    "\n",
    "def h(x):\n",
    "    bits = 28\n",
    "    return murmurhash3_32(x) % 2**bits\n",
    "\n",
    "def h_with_colname(x, y):\n",
    "    return h(x) + h(y)\n",
    "\n",
    "def do_as_selivanov_does_2(x):\n",
    "    return Row(country=x.country+h('country'),\n",
    "               dma=x.dma+h('dma'),\n",
    "               geo_location=x.geo_location+h('geo_location'),\n",
    "               state=x.state+h('state'),\n",
    "               uuid=x.uuid+h('uuid'),\n",
    "               display_id=h_with_colname(int(x.display_id), 'display_id'),\n",
    "               display_id_no_hash=int(x.display_id),\n",
    "               document_id=h_with_colname(int(x.document_id), 'document_id'),\n",
    "               ad_id=h_with_colname(int(x.ad_id), 'ad_id'),\n",
    "               ad_id_no_hash=int(x.ad_id),\n",
    "               promo_document_id=h_with_colname(int(x.promo_document_id), 'promo_document_id'),\n",
    "               campaign_id=h_with_colname(int(x.campaign_id), 'campaign_id'),\n",
    "               advertiser_id=h_with_colname(int(x.advertiser_id), 'advertiser_id'),\n",
    "               platform=int(x.platform),\n",
    "               timestamp=int(x.timestamp))\n",
    "               \n",
    "\n",
    "ecpj_hashed_test = events_clicks_prom_joined_test.rdd \\\n",
    "    .map(do_as_selivanov_does_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 64 ms, sys: 8 ms, total: 72 ms\n",
      "Wall time: 5min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# much faster thanks to conversion back to DataFrame (works for simple python collections in columns)\n",
    "ss.createDataFrame(ecpj_hashed_test).write.mode(\"overwrite\").parquet(\"/task1/ecpj_hashed_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ss.read.parquet(\"/task1/ecpj_hashed_test\").registerTempTable(\"ecpj_hashed_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ad_id=307775314, ad_id_no_hash=7343, advertiser_id=506710073, campaign_id=301792321, country=308936875, display_id=496221867, display_id_no_hash=16874679, dma=456348757, document_id=252442268, geo_location=264330675, platform=2, promo_document_id=462589401, state=411831051, timestamp=38382, uuid=259021810)]"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecpj_hashed_test.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ecpj_hashed_test.saveAsTextFile('/task1/ecpj_hashed_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def copy_text_to_local(hdfs_path, local_path):\n",
    "    if os.path.exists(local_path):\n",
    "        shutil.rmtree(local_path)\n",
    "    os.mkdir(local_path)\n",
    "    os.system('hadoop fs -copyToLocal \"{0}/*\" {1}'.format(hdfs_path, local_path))\n",
    "    os.system('cat {0}/part-* > {1}'.format(local_path, local_path + \"/merged.txt\"))\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "CPU times: user 32 ms, sys: 8 ms, total: 40 ms\n",
      "Wall time: 5min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "copy_text_to_local(\"/task1/ecpj_hashed_test.txt\", \"/data/ecpj_hashed_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(ad_id=253319144, ad_id_no_hash=174547, advertiser_id=440806606, campaign_id=199403314, country=308936875, display_id=377302092, display_id_no_hash=23119602, dma=379001842, document_id=139141068, geo_location=229219811, platform=2, promo_document_id=511012270, state=205914184, timestamp=1295966561, uuid=78228505)\r\n",
      "Row(ad_id=294458570, ad_id_no_hash=192149, advertiser_id=257449846, campaign_id=308214969, country=308936875, display_id=377302092, display_id_no_hash=23119602, dma=379001842, document_id=139141068, geo_location=229219811, platform=2, promo_document_id=393154076, state=205914184, timestamp=1295966561, uuid=78228505)\r\n",
      "Row(ad_id=316468025, ad_id_no_hash=539951, advertiser_id=427254936, campaign_id=170294627, country=308936875, display_id=377302092, display_id_no_hash=23119602, dma=379001842, document_id=139141068, geo_location=229219811, platform=2, promo_document_id=271542147, state=205914184, timestamp=1295966561, uuid=78228505)\r\n",
      "Row(ad_id=367688901, ad_id_no_hash=566224, advertiser_id=292110095, campaign_id=182069611, country=308936875, display_id=377302092, display_id_no_hash=23119602, dma=379001842, document_id=139141068, geo_location=229219811, platform=2, promo_document_id=413005504, state=205914184, timestamp=1295966561, uuid=78228505)\r\n"
     ]
    }
   ],
   "source": [
    "!tail -n 4 /data/ecpj_hashed_test.txt/merged.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!head /data/ecpj_hashed_test.txt/merged.txt > /data/check.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function    \n",
    "\n",
    "def txt_to_vw(in_file, out_file, no_hash_file):\n",
    "    with open(in_file, 'r') as in_f, \\\n",
    "         open(out_file, 'w') as out_f, \\\n",
    "         open(no_hash_file, 'w') as no_hash_f:\n",
    "        for l in in_f:\n",
    "            pairs = (l.split('('))[1].split(')')[0].split(', ')\n",
    "            d = dict(p.split('=') for p in pairs)\n",
    "\n",
    "            ad_id = d.pop('ad_id_no_hash')\n",
    "            display_id = d.pop('display_id_no_hash')\n",
    "            print('{}, {}'.format(display_id, ad_id), file=no_hash_f)\n",
    "\n",
    "            s = '| ' + ' '.join('{}:1'.format(v) for k,v in d.items())\n",
    "            print(s, file=out_f)\n",
    "\n",
    "txt_to_vw('/data/check.txt',\n",
    "          '/data/vw_in_test.txt',\n",
    "          '/data/no_hash_rows_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 496221867:1 307775314:1 259021810:1 38382:1 456348757:1 308936875:1 301792321:1 506710073:1 2:1 411831051:1 264330675:1 462589401:1 252442268:1\r\n",
      "| 496221867:1 165121396:1 259021810:1 38382:1 456348757:1 308936875:1 287141130:1 326816900:1 2:1 411831051:1 264330675:1 398544910:1 252442268:1\r\n",
      "| 496221867:1 165174384:1 259021810:1 38382:1 456348757:1 308936875:1 188603283:1 364587572:1 2:1 411831051:1 264330675:1 484004916:1 252442268:1\r\n",
      "| 496221867:1 287148001:1 259021810:1 38382:1 456348757:1 308936875:1 349360155:1 499834666:1 2:1 411831051:1 264330675:1 517377613:1 252442268:1\r\n",
      "| 336716434:1 126561095:1 105049090:1 170597:1 289378491:1 308936875:1 271510132:1 506710073:1 2:1 380123499:1 287315324:1 403937613:1 264569148:1\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 /data/vw_in_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train VW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "https://github.com/JohnLangford/vowpal_wabbit/wiki/Command-line-arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Now let's use coeffs from original solution by Selivanov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using l1 regularization = 1\n",
      "using l2 regularization = 1\n",
      "final_regressor = /data/model\n",
      "Enabling FTRL based optimization\n",
      "Algorithm used: Proximal-FTRL\n",
      "ftrl_alpha = 0.05\n",
      "ftrl_beta = 0.5\n",
      "Num weight bits = 24\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "creating cache_file = /data/vw_in.txt.cache\n",
      "Reading datafile = /data/vw_in.txt\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.443453 0.443453      8000000      8000000.0  -1.0000  -1.3975       14\n",
      "0.441102 0.438750     16000000     16000000.0   1.0000  -2.4376       14\n",
      "0.439983 0.437745     24000000     24000000.0  -1.0000  -1.8304       14\n",
      "0.439204 0.436867     32000000     32000000.0  -1.0000  -1.6973       14\n",
      "0.438689 0.436627     40000000     40000000.0  -1.0000  -2.1245       14\n",
      "0.438224 0.435901     48000000     48000000.0  -1.0000  -2.8373       14\n",
      "0.437883 0.435838     56000000     56000000.0  -1.0000  -4.0696       14\n",
      "0.437611 0.435702     64000000     64000000.0  -1.0000  -3.0913       14\n",
      "0.437365 0.435397     72000000     72000000.0  -1.0000  -2.8828       14\n",
      "0.437184 0.435561     80000000     80000000.0   1.0000  -2.3344       14\n",
      "\n",
      "finished run\n",
      "number of examples = 87141731\n",
      "weighted example sum = 87141731.000000\n",
      "weighted label sum = -53392545.000000\n",
      "average loss = 0.437043\n",
      "best constant = -1.426495\n",
      "best constant's loss = 0.491466\n",
      "total feature number = 1219984234\n",
      "only testing\n",
      "predictions = /data/test_predictions.txt\n",
      "Num weight bits = 24\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = /data/vw_in_test.txt\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "    n.a.     n.a.      1000000      1000000.0  unknown   0.4802       14\n",
      "    n.a.     n.a.      2000000      2000000.0  unknown   0.2501       14\n",
      "    n.a.     n.a.      3000000      3000000.0  unknown   0.2384       14\n",
      "    n.a.     n.a.      4000000      4000000.0  unknown   0.2208       14\n",
      "    n.a.     n.a.      5000000      5000000.0  unknown   0.1582       14\n",
      "    n.a.     n.a.      6000000      6000000.0  unknown   0.0158       14\n",
      "    n.a.     n.a.      7000000      7000000.0  unknown   0.4267       14\n",
      "    n.a.     n.a.      8000000      8000000.0  unknown   0.1085       14\n",
      "    n.a.     n.a.      9000000      9000000.0  unknown   0.0968       14\n",
      "    n.a.     n.a.     10000000     10000000.0  unknown   0.1931       14\n",
      "    n.a.     n.a.     11000000     11000000.0  unknown   0.1111       14\n",
      "    n.a.     n.a.     12000000     12000000.0  unknown   0.1771       14\n",
      "    n.a.     n.a.     13000000     13000000.0  unknown   0.0127       14\n",
      "    n.a.     n.a.     14000000     14000000.0  unknown   0.0183       14\n",
      "    n.a.     n.a.     15000000     15000000.0  unknown   0.2515       14\n",
      "    n.a.     n.a.     16000000     16000000.0  unknown   0.2956       14\n",
      "    n.a.     n.a.     17000000     17000000.0  unknown   0.2127       14\n",
      "    n.a.     n.a.     18000000     18000000.0  unknown   0.1969       14\n",
      "    n.a.     n.a.     19000000     19000000.0  unknown   0.0411       14\n",
      "    n.a.     n.a.     20000000     20000000.0  unknown   0.0618       14\n",
      "    n.a.     n.a.     21000000     21000000.0  unknown   0.0621       14\n",
      "    n.a.     n.a.     22000000     22000000.0  unknown   0.3073       14\n",
      "    n.a.     n.a.     23000000     23000000.0  unknown   0.3913       14\n",
      "    n.a.     n.a.     24000000     24000000.0  unknown   0.2105       14\n",
      "    n.a.     n.a.     25000000     25000000.0  unknown   0.1584       14\n",
      "    n.a.     n.a.     26000000     26000000.0  unknown   0.1393       14\n",
      "    n.a.     n.a.     27000000     27000000.0  unknown   0.1899       14\n",
      "    n.a.     n.a.     28000000     28000000.0  unknown   0.1142       14\n",
      "    n.a.     n.a.     29000000     29000000.0  unknown   0.2122       14\n",
      "    n.a.     n.a.     30000000     30000000.0  unknown   0.0702       14\n",
      "    n.a.     n.a.     31000000     31000000.0  unknown   0.1416       14\n",
      "    n.a.     n.a.     32000000     32000000.0  unknown   0.0743       14\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 32225162\n",
      "passes used = 1\n",
      "weighted example sum = 32225162.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = n.a.\n",
      "total feature number = 451152268\n"
     ]
    }
   ],
   "source": [
    "!LD_LIBRARY_PATH=/usr/local/lib vw -d /data/vw_in.txt -b 24 -c -k --ftrl --ftrl_alpha 0.05 --ftrl_beta 0.5 --l1 1 --l2 1 --passes 1 -f /data/model --holdout_off --loss_function logistic --random_seed 42 --progress 8000000 \n",
    "!LD_LIBRARY_PATH=/usr/local/lib vw -d /data/vw_in_test.txt -i /data/model -t -k -p /data/test_predictions.txt --progress 1000000 --link=logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.190996\r\n",
      "0.094548\r\n",
      "0.176351\r\n",
      "0.185417\r\n",
      "0.193987\r\n"
     ]
    }
   ],
   "source": [
    "# !head -n 5 /data/test_predictions.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !head /data/test_predictions.txt > /data/check.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's merge two tables columnwise using python script\n",
    "from itertools import izip\n",
    "from __future__ import print_function\n",
    "\n",
    "def merge_pred_no_hash(preds_file, no_hash_file, ans_file):\n",
    "    with open(preds_file, 'r') as pf, \\\n",
    "         open(no_hash_file, 'r') as nh_f, \\\n",
    "         open(ans_file, 'w') as af:\n",
    "        for x, y in izip(nh_f, pf):\n",
    "            print('{}, {}'.format(x.strip(), y.strip()), file=af)\n",
    "\n",
    "merge_pred_no_hash('/data/test_predictions.txt',\n",
    "                   '/data/no_hash_rows_test.txt',\n",
    "                   '/data/no_hash_preds.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16874679, 7343, 0.190996\r\n",
      "16874679, 137124, 0.094548\r\n",
      "16874679, 147706, 0.176351\r\n",
      "16874679, 304154, 0.185417\r\n",
      "16875035, 3784, 0.193987\r\n",
      "16875035, 33142, 0.232907\r\n",
      "16875035, 89504, 0.348130\r\n",
      "16875035, 111980, 0.297726\r\n",
      "16875035, 149047, 0.283822\r\n",
      "16875035, 171530, 0.337937\r\n"
     ]
    }
   ],
   "source": [
    "!head /data/no_hash_preds.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16874679, 7343, 0.167151\r\n",
      "16874679, 137124, 0.076552\r\n",
      "16874679, 147706, 0.169222\r\n",
      "16874679, 304154, 0.176729\r\n",
      "16875035, 3784, 0.226063\r\n",
      "16875035, 33142, 0.252485\r\n",
      "16875035, 89504, 0.419533\r\n",
      "16875035, 111980, 0.306840\r\n",
      "16875035, 149047, 0.332894\r\n",
      "16875035, 171530, 0.347986\r\n"
     ]
    }
   ],
   "source": [
    "!head /data/no_hash_preds.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/05/23 14:11:12 INFO fs.TrashPolicyDefault: Moved: 'hdfs://cluster1:8020/task1/no_hash_preds.txt' to trash at: hdfs://cluster1:8020/user/ubuntu/.Trash/Current/task1/no_hash_preds.txt\r\n"
     ]
    }
   ],
   "source": [
    "# !hadoop fs -rm /task1/no_hash_preds.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/05/23 14:11:33 INFO fs.TrashPolicyDefault: TrashPolicyDefault#deleteCheckpoint for trashRoot: hdfs://cluster1:8020/user/ubuntu/.Trash\n",
      "18/05/23 14:11:33 INFO fs.TrashPolicyDefault: TrashPolicyDefault#deleteCheckpoint for trashRoot: hdfs://cluster1:8020/user/ubuntu/.Trash\n",
      "18/05/23 14:11:33 INFO fs.TrashPolicyDefault: TrashPolicyDefault#createCheckpoint for trashRoot: hdfs://cluster1:8020/user/ubuntu/.Trash\n",
      "18/05/23 14:11:33 INFO fs.TrashPolicyDefault: Created trash checkpoint: /user/ubuntu/.Trash/180523141133\n"
     ]
    }
   ],
   "source": [
    "# !hadoop fs -expunge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# !hadoop fs -put /data/test_predictions.txt /task1/test_predictions.txt\n",
    "!hadoop fs -put /data/no_hash_preds.txt /task1/no_hash_preds.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = ss.read.csv('/task1/no_hash_preds.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+---------+\n",
      "|     _c0|    _c1|      _c2|\n",
      "+--------+-------+---------+\n",
      "|16874679|   7343| 0.167151|\n",
      "|16874679| 137124| 0.076552|\n",
      "|16874679| 147706| 0.169222|\n",
      "+--------+-------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# c0 - display_id\n",
    "# c1 - ad_id\n",
    "# c2 - probability of being clicked\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "pred_sorted = predictions.sort(['_c0', '_c2'], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+---------+\n",
      "|     _c0|    _c1|      _c2|\n",
      "+--------+-------+---------+\n",
      "|16874594| 172888| 0.313364|\n",
      "|16874594| 170392| 0.306634|\n",
      "|16874594| 162754| 0.208949|\n",
      "|16874594| 150083| 0.064795|\n",
      "|16874594|  66758| 0.053696|\n",
      "|16874594| 180797| 0.027205|\n",
      "|16874595|   8846| 0.348094|\n",
      "|16874595| 143982| 0.189634|\n",
      "|16874595|  30609| 0.075779|\n",
      "|16874596| 289915| 0.390936|\n",
      "+--------+-------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_sorted.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+---------+\n",
      "|     _c0|    _c1|      _c2|\n",
      "+--------+-------+---------+\n",
      "|16874594| 170392| 0.325347|\n",
      "|16874594| 172888| 0.316497|\n",
      "|16874594| 162754| 0.223360|\n",
      "|16874594|  66758| 0.077509|\n",
      "|16874594| 150083| 0.064767|\n",
      "|16874594| 180797| 0.035961|\n",
      "|16874595|   8846| 0.290929|\n",
      "|16874595| 143982| 0.155837|\n",
      "|16874595|  30609| 0.066543|\n",
      "|16874596| 289915| 0.369923|\n",
      "|16874596| 289122| 0.368243|\n",
      "|16874596|  11430| 0.363113|\n",
      "|16874596| 132820| 0.341290|\n",
      "|16874596|  57197| 0.150066|\n",
      "|16874596| 153260| 0.116894|\n",
      "|16874596| 173005| 0.072445|\n",
      "|16874596| 288385| 0.060890|\n",
      "|16874597| 285834| 0.229594|\n",
      "|16874597| 305790| 0.166391|\n",
      "|16874597| 308836| 0.161904|\n",
      "|16874597| 143981| 0.145204|\n",
      "|16874597| 182039| 0.116135|\n",
      "|16874597| 155945| 0.108001|\n",
      "|16874597| 180965| 0.098141|\n",
      "|16874597| 137858| 0.067286|\n",
      "|16874598| 145937| 0.165656|\n",
      "|16874598| 335632| 0.074875|\n",
      "|16874598|  67292| 0.057873|\n",
      "|16874598| 250082| 0.037220|\n",
      "|16874599| 173130| 0.323628|\n",
      "|16874599|  91681| 0.308299|\n",
      "|16874599| 210516| 0.259783|\n",
      "|16874599| 213116| 0.243303|\n",
      "|16874599| 296295| 0.241135|\n",
      "|16874599| 163776| 0.076273|\n",
      "|16874600|  30682| 0.356869|\n",
      "|16874600| 133050| 0.304187|\n",
      "|16874600|  70529| 0.301915|\n",
      "|16874600|  57591| 0.277483|\n",
      "|16874600|   2150| 0.226782|\n",
      "|16874600| 114836| 0.096221|\n",
      "|16874601| 190713| 0.238290|\n",
      "|16874601|  92003| 0.209385|\n",
      "|16874601| 129490| 0.164352|\n",
      "|16874601|  14082| 0.114659|\n",
      "|16874601| 140942| 0.110106|\n",
      "|16874601| 118470| 0.088784|\n",
      "|16874602| 154918| 0.467296|\n",
      "|16874602| 281563| 0.240207|\n",
      "|16874602| 269017| 0.123736|\n",
      "+--------+-------+---------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pred_sorted.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_grouped = pred_sorted.groupby('_c0').agg(F.collect_list(\"_c1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_gr_sorted = pred_grouped.sort(['_c0'], ascending=[True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|     _c0|   collect_list(_c1)|\n",
      "+--------+--------------------+\n",
      "|16874594|[ 172888,  170392...|\n",
      "|16874595|[ 8846,  143982, ...|\n",
      "|16874596|[ 289915,  289122...|\n",
      "+--------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_gr_sorted.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|     _c0|   collect_list(_c1)|\n",
      "+--------+--------------------+\n",
      "|16874594|[ 170392,  172888...|\n",
      "|16874595|[ 8846,  143982, ...|\n",
      "|16874596|[ 289915,  289122...|\n",
      "+--------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_gr_sorted.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def stringify(arr):\n",
    "    arr = map(str, arr)\n",
    "    return ' '.join(map(str.strip, arr))\n",
    "\n",
    "udf_stringify = func.udf(stringify, returnType=StringType())\n",
    "\n",
    "pred_final = pred_gr_sorted \\\n",
    "    .withColumn('ad_id', udf_stringify('collect_list(_c1)')) \\\n",
    "    .withColumnRenamed('_c0', 'display_id') \\\n",
    "    .drop('collect_list(_c1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|display_id|               ad_id|\n",
      "+----------+--------------------+\n",
      "|  16874594|170392 172888 162...|\n",
      "|  16874595|   8846 143982 30609|\n",
      "|  16874596|289915 289122 114...|\n",
      "+----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_final.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|display_id|               ad_id|\n",
      "+----------+--------------------+\n",
      "|  16874594|172888 170392 162...|\n",
      "|  16874595|   8846 143982 30609|\n",
      "|  16874596|289915 289122 114...|\n",
      "+----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_final.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_final.write.mode(\"overwrite\").option(\"header\",\"true\").csv('/task1/submission_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def copy_text_to_local(hdfs_path, local_path):\n",
    "    if os.path.exists(local_path):\n",
    "        shutil.rmtree(local_path)\n",
    "    os.mkdir(local_path)\n",
    "    os.system('hadoop fs -copyToLocal \"{0}/*\" {1}'.format(hdfs_path, local_path))\n",
    "    os.system('head -n 1 {0}/part-00001* > {1}'.format(local_path, local_path + \"/merged.csv\"))\n",
    "    os.system('find {0} -name \"part*.csv\" | xargs -n 1 tail -n +2 >> {1}' \\\n",
    "              .format(local_path, local_path + \"/merged.csv\"))\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "CPU times: user 0 ns, sys: 12 ms, total: 12 ms\n",
      "Wall time: 4.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "copy_text_to_local(\"/task1/submission_1.csv\",\\\n",
    "                   \"/data/submission_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22235616,30682 155249 183640 542754 153413 153658\r\n",
      "22235617,134692 279921 118470 123730\r\n",
      "22235618,109018 117524 2820 147014 37981 161989\r\n"
     ]
    }
   ],
   "source": [
    "!sed -n '56394, 56396p; 56397q' /data/submission_2.csv/merged.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display_id,ad_id\r\n",
      "18929906,463707 117305 158632\r\n",
      "18929907,184220 166286 57791 285722 163843 156669 125865 129600 179761\r\n",
      "18929908,396884 99515 139376 155251 159539 303433\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 4 /data/submission_2.csv/merged.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display_id,ad_id\r\n",
      "17960876,155607 319252 304273 141437\r\n",
      "17960877,257366 198149 173411 123742\r\n",
      "17960878,150609 148954 81742 246879 125218 280782\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 4 /data/submission_1.csv/merged.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display_id,ad_id\r\n",
      "16874594,66758 150083 162754 170392 172888 180797\r\n",
      "16874595,8846 30609 143982\r\n",
      "16874596,11430 57197 132820 153260 173005 288385 289122 289915\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 4 /data/sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ss.sql('select * from sample_submission').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/urllib3/util/ssl_.py:339: SNIMissingWarning: An HTTPS request has been made, but the SNI (Subject Name Indication) extension to TLS is not available on this platform. This may cause the server to present an incorrect TLS certificate, which can cause validation failures. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  SNIMissingWarning\n",
      "/usr/local/lib/python2.7/dist-packages/urllib3/util/ssl_.py:137: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecurePlatformWarning\n",
      "Successfully submitted to Outbrain Click Prediction"
     ]
    }
   ],
   "source": [
    "# !kaggle competitions submit -c outbrain-click-prediction -f /data/sample_submission.csv -m \"Sample submission\"\n",
    "!kaggle competitions submit -c outbrain-click-prediction -f /data/submission_2.csv/merged.csv -m \"Submission 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read_vw_predictions(p):\n",
    "    y_pred = []\n",
    "    with open(p, \"r\") as f:\n",
    "        for line in f:\n",
    "            y_pred.append(float(line.split()[0]))\n",
    "    return np.array(y_pred)\n",
    "\n",
    "y_pred = read_vw_predictions(\"/data/test_predictions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_vw_y_true(p):\n",
    "    y_true = []\n",
    "    with open(p, \"r\") as f:\n",
    "        for line in f:\n",
    "            y_true.append(float(line.partition(\" \")[0]))\n",
    "    return np.array(y_true)\n",
    "\n",
    "y_true = get_vw_y_true(\"/data/test.txt/merged.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-265-e1fe70993a93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.pyc\u001b[0m in \u001b[0;36mlog_loss\u001b[0;34m(y_true, y_pred, eps, normalize, sample_weight, labels)\u001b[0m\n\u001b[1;32m   1606\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mlogarithm\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnatural\u001b[0m \u001b[0mlogarithm\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m     \"\"\"\n\u001b[0;32m-> 1608\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1609\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    414\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m                              % (n_samples, shape_repr, ensure_min_samples,\n\u001b[0;32m--> 416\u001b[0;31m                                 context))\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "log_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "roc_auc_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# My code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Copy data from HDFS to cluster1 machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will run vowpal wabbit **locally**, need to copy data from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def copy_text_to_local(hdfs_path, local_path):\n",
    "    if os.path.exists(local_path):\n",
    "        shutil.rmtree(local_path)\n",
    "    os.mkdir(local_path)\n",
    "    os.system('hadoop fs -copyToLocal \"{0}/*\" {1}'.format(hdfs_path, local_path))\n",
    "    os.system('cat {0}/part-* > {1}'.format(local_path, local_path + \"/merged.txt\"))\n",
    "    print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 2.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "copy_text_to_local(\"/task1/train.txt\", \"/data/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ubuntu ubuntu 0 May 18 13:56 /data/train.txt/merged.txt\r\n"
     ]
    }
   ],
   "source": [
    "! ls -lh /data/train.txt/merged.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "copy_text_to_local(\"/task1/test.txt\", \"/data/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ubuntu ubuntu 0 May 18 13:56 /data/test.txt/merged.txt\r\n"
     ]
    }
   ],
   "source": [
    "! ls -lh /data/test.txt/merged.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Install VW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  python-chardet-whl python-colorama python-colorama-whl python-distlib\n",
      "  python-distlib-whl python-html5lib python-html5lib-whl python-pip-whl\n",
      "  python-requests-whl python-setuptools-whl python-six-whl python-urllib3-whl\n",
      "  python-wheel python3-pkg-resources\n",
      "Use 'apt-get autoremove' to remove them.\n",
      "The following extra packages will be installed:\n",
      "  autoconf autotools-dev libboost-program-options1.54-dev\n",
      "  libboost-program-options1.54.0 libboost-python1.54-dev libboost-python1.54.0\n",
      "  libboost1.54-dev libltdl-dev\n",
      "Suggested packages:\n",
      "  autoconf2.13 autoconf-archive gnu-standards autoconf-doc gettext\n",
      "  libboost1.54-doc python-pyste libboost-atomic1.54-dev\n",
      "  libboost-chrono1.54-dev libboost-context1.54-dev libboost-coroutine.54-dev\n",
      "  libboost-date-time1.54-dev libboost-exception1.54-dev\n",
      "  libboost-filesystem1.54-dev libboost-graph1.54-dev\n",
      "  libboost-graph-parallel1.54-dev libboost-iostreams1.54-dev\n",
      "  libboost-locale1.54-dev libboost-log.54-dev libboost-math1.54-dev\n",
      "  libboost-mpi1.54-dev libboost-mpi-python1.54-dev libboost-random1.54-dev\n",
      "  libboost-regex1.54-dev libboost-serialization1.54-dev\n",
      "  libboost-signals1.54-dev libboost-system1.54-dev libboost-test1.54-dev\n",
      "  libboost-thread1.54-dev libboost-timer1.54-dev libboost-wave1.54-dev\n",
      "  libboost1.54-tools-dev libmpfrc++-dev libntl-dev libtool-doc automaken\n",
      "  gfortran fortran95-compiler gcj-jdk\n",
      "The following NEW packages will be installed:\n",
      "  autoconf automake autotools-dev libboost-program-options-dev\n",
      "  libboost-program-options1.54-dev libboost-program-options1.54.0\n",
      "  libboost-python-dev libboost-python1.54-dev libboost-python1.54.0\n",
      "  libboost1.54-dev libltdl-dev libtool m4 zlib1g-dev\n",
      "0 upgraded, 14 newly installed, 0 to remove and 198 not upgraded.\n",
      "Need to get 7777 kB of archives.\n",
      "After this operation, 107 MB of additional disk space will be used.\n",
      "Get:1 http://azure.archive.ubuntu.com/ubuntu/ trusty-updates/main libboost-program-options1.54.0 amd64 1.54.0-4ubuntu3.1 [115 kB]\n",
      "Get:2 http://azure.archive.ubuntu.com/ubuntu/ trusty-updates/main libboost-python1.54.0 amd64 1.54.0-4ubuntu3.1 [122 kB]\n",
      "Get:3 http://azure.archive.ubuntu.com/ubuntu/ trusty/main m4 amd64 1.4.17-2ubuntu1 [195 kB]\n",
      "Get:4 http://azure.archive.ubuntu.com/ubuntu/ trusty/main autoconf all 2.69-6 [322 kB]\n",
      "Get:5 http://azure.archive.ubuntu.com/ubuntu/ trusty/main autotools-dev all 20130810.1 [44.3 kB]\n",
      "Get:6 http://azure.archive.ubuntu.com/ubuntu/ trusty/main automake all 1:1.14.1-2ubuntu1 [510 kB]\n",
      "Get:7 http://azure.archive.ubuntu.com/ubuntu/ trusty-updates/main libboost1.54-dev amd64 1.54.0-4ubuntu3.1 [5682 kB]\n",
      "Get:8 http://azure.archive.ubuntu.com/ubuntu/ trusty-updates/main libboost-program-options1.54-dev amd64 1.54.0-4ubuntu3.1 [133 kB]\n",
      "Get:9 http://azure.archive.ubuntu.com/ubuntu/ trusty/main libboost-program-options-dev amd64 1.54.0.1ubuntu1 [2840 B]\n",
      "Get:10 http://azure.archive.ubuntu.com/ubuntu/ trusty-updates/main libboost-python1.54-dev amd64 1.54.0-4ubuntu3.1 [119 kB]\n",
      "Get:11 http://azure.archive.ubuntu.com/ubuntu/ trusty/main libboost-python-dev amd64 1.54.0.1ubuntu1 [3204 B]\n",
      "Get:12 http://azure.archive.ubuntu.com/ubuntu/ trusty/main libltdl-dev amd64 2.4.2-1.7ubuntu1 [157 kB]\n",
      "Get:13 http://azure.archive.ubuntu.com/ubuntu/ trusty/main libtool amd64 2.4.2-1.7ubuntu1 [188 kB]\n",
      "Get:14 http://azure.archive.ubuntu.com/ubuntu/ trusty/main zlib1g-dev amd64 1:1.2.8.dfsg-1ubuntu1 [183 kB]\n",
      "Fetched 7777 kB in 0s (32.9 MB/s)   \n",
      "perl: warning: Setting locale failed.\n",
      "perl: warning: Please check that your locale settings:\n",
      "\tLANGUAGE = (unset),\n",
      "\tLC_ALL = (unset),\n",
      "\tLC_TIME = \"ru_RU.UTF-8\",\n",
      "\tLC_MONETARY = \"ru_RU.UTF-8\",\n",
      "\tLC_ADDRESS = \"ru_RU.UTF-8\",\n",
      "\tLC_TELEPHONE = \"ru_RU.UTF-8\",\n",
      "\tLC_NAME = \"ru_RU.UTF-8\",\n",
      "\tLC_MEASUREMENT = \"ru_RU.UTF-8\",\n",
      "\tLC_IDENTIFICATION = \"ru_RU.UTF-8\",\n",
      "\tLC_NUMERIC = \"ru_RU.UTF-8\",\n",
      "\tLC_PAPER = \"ru_RU.UTF-8\",\n",
      "\tLANG = \"en_US.UTF-8\"\n",
      "    are supported and installed on your system.\n",
      "perl: warning: Falling back to the standard locale (\"C\").\n",
      "locale: Cannot set LC_ALL to default locale: No such file or directory\n",
      "Selecting previously unselected package libboost-program-options1.54.0:amd64.\n",
      "(Reading database ... 58377 files and directories currently installed.)\n",
      "Preparing to unpack .../libboost-program-options1.54.0_1.54.0-4ubuntu3.1_amd64.deb ...\n",
      "Unpacking libboost-program-options1.54.0:amd64 (1.54.0-4ubuntu3.1) ...\n",
      "Selecting previously unselected package libboost-python1.54.0:amd64.\n",
      "Preparing to unpack .../libboost-python1.54.0_1.54.0-4ubuntu3.1_amd64.deb ...\n",
      "Unpacking libboost-python1.54.0:amd64 (1.54.0-4ubuntu3.1) ...\n",
      "Selecting previously unselected package m4.\n",
      "Preparing to unpack .../m4_1.4.17-2ubuntu1_amd64.deb ...\n",
      "Unpacking m4 (1.4.17-2ubuntu1) ...\n",
      "Selecting previously unselected package autoconf.\n",
      "Preparing to unpack .../autoconf_2.69-6_all.deb ...\n",
      "Unpacking autoconf (2.69-6) ...\n",
      "Selecting previously unselected package autotools-dev.\n",
      "Preparing to unpack .../autotools-dev_20130810.1_all.deb ...\n",
      "Unpacking autotools-dev (20130810.1) ...\n",
      "Selecting previously unselected package automake.\n",
      "Preparing to unpack .../automake_1%3a1.14.1-2ubuntu1_all.deb ...\n",
      "Unpacking automake (1:1.14.1-2ubuntu1) ...\n",
      "Selecting previously unselected package libboost1.54-dev.\n",
      "Preparing to unpack .../libboost1.54-dev_1.54.0-4ubuntu3.1_amd64.deb ...\n",
      "Unpacking libboost1.54-dev (1.54.0-4ubuntu3.1) ...\n",
      "Selecting previously unselected package libboost-program-options1.54-dev:amd64.\n",
      "Preparing to unpack .../libboost-program-options1.54-dev_1.54.0-4ubuntu3.1_amd64.deb ...\n",
      "Unpacking libboost-program-options1.54-dev:amd64 (1.54.0-4ubuntu3.1) ...\n",
      "Selecting previously unselected package libboost-program-options-dev:amd64.\n",
      "Preparing to unpack .../libboost-program-options-dev_1.54.0.1ubuntu1_amd64.deb ...\n",
      "Unpacking libboost-program-options-dev:amd64 (1.54.0.1ubuntu1) ...\n",
      "Selecting previously unselected package libboost-python1.54-dev:amd64.\n",
      "Preparing to unpack .../libboost-python1.54-dev_1.54.0-4ubuntu3.1_amd64.deb ...\n",
      "Unpacking libboost-python1.54-dev:amd64 (1.54.0-4ubuntu3.1) ...\n",
      "Selecting previously unselected package libboost-python-dev.\n",
      "Preparing to unpack .../libboost-python-dev_1.54.0.1ubuntu1_amd64.deb ...\n",
      "Unpacking libboost-python-dev (1.54.0.1ubuntu1) ...\n",
      "Selecting previously unselected package libltdl-dev:amd64.\n",
      "Preparing to unpack .../libltdl-dev_2.4.2-1.7ubuntu1_amd64.deb ...\n",
      "Unpacking libltdl-dev:amd64 (2.4.2-1.7ubuntu1) ...\n",
      "Selecting previously unselected package libtool.\n",
      "Preparing to unpack .../libtool_2.4.2-1.7ubuntu1_amd64.deb ...\n",
      "Unpacking libtool (2.4.2-1.7ubuntu1) ...\n",
      "Selecting previously unselected package zlib1g-dev:amd64.\n",
      "Preparing to unpack .../zlib1g-dev_1%3a1.2.8.dfsg-1ubuntu1_amd64.deb ...\n",
      "Unpacking zlib1g-dev:amd64 (1:1.2.8.dfsg-1ubuntu1) ...\n",
      "Processing triggers for man-db (2.6.7.1-1ubuntu1) ...\n",
      "Processing triggers for install-info (5.2.0.dfsg.1-2) ...\n",
      "Setting up libboost-program-options1.54.0:amd64 (1.54.0-4ubuntu3.1) ...\n",
      "Setting up libboost-python1.54.0:amd64 (1.54.0-4ubuntu3.1) ...\n",
      "Setting up m4 (1.4.17-2ubuntu1) ...\n",
      "Setting up autoconf (2.69-6) ...\n",
      "Setting up autotools-dev (20130810.1) ...\n",
      "Setting up automake (1:1.14.1-2ubuntu1) ...\n",
      "update-alternatives: using /usr/bin/automake-1.14 to provide /usr/bin/automake (automake) in auto mode\n",
      "Setting up libboost1.54-dev (1.54.0-4ubuntu3.1) ...\n",
      "Setting up libboost-program-options1.54-dev:amd64 (1.54.0-4ubuntu3.1) ...\n",
      "Setting up libboost-program-options-dev:amd64 (1.54.0.1ubuntu1) ...\n",
      "Setting up libboost-python1.54-dev:amd64 (1.54.0-4ubuntu3.1) ...\n",
      "Setting up libboost-python-dev (1.54.0.1ubuntu1) ...\n",
      "Setting up libltdl-dev:amd64 (2.4.2-1.7ubuntu1) ...\n",
      "Setting up libtool (2.4.2-1.7ubuntu1) ...\n",
      "Setting up zlib1g-dev:amd64 (1:1.2.8.dfsg-1ubuntu1) ...\n",
      "Processing triggers for libc-bin (2.19-0ubuntu6.9) ...\n"
     ]
    }
   ],
   "source": [
    "! sudo apt-get install libboost-program-options-dev zlib1g-dev libboost-python-dev libtool m4 automake -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-05-18 13:57:16--  https://github.com/JohnLangford/vowpal_wabbit/archive/8.2.0.tar.gz\n",
      "Resolving github.com (github.com)... 192.30.253.113, 192.30.253.112\n",
      "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://codeload.github.com/JohnLangford/vowpal_wabbit/tar.gz/8.2.0 [following]\n",
      "--2018-05-18 13:57:16--  https://codeload.github.com/JohnLangford/vowpal_wabbit/tar.gz/8.2.0\n",
      "Resolving codeload.github.com (codeload.github.com)... 192.30.253.121, 192.30.253.120\n",
      "Connecting to codeload.github.com (codeload.github.com)|192.30.253.121|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/x-gzip]\n",
      "Saving to: '8.2.0.tar.gz'\n",
      "\n",
      "    [     <=>                               ] 13,391,191  12.2MB/s   in 1.0s   \n",
      "\n",
      "2018-05-18 13:57:18 (12.2 MB/s) - '8.2.0.tar.gz' saved [13391191]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://github.com/JohnLangford/vowpal_wabbit/archive/8.2.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "! tar -xzf 8.2.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "cd vowpal_wabbit-8.2.0\n",
    "\n",
    "./autogen.sh\n",
    "\n",
    "make -j4\n",
    "\n",
    "make test -j4\n",
    "\n",
    "sudo make install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train VW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "https://github.com/JohnLangford/vowpal_wabbit/wiki/Command-line-arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "! head -n2 /data/train.txt/merged.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_regressor = /data/model\n",
      "Enabling FTRL based optimization\n",
      "Algorithm used: Proximal-FTRL\n",
      "ftrl_alpha = 0.005\n",
      "ftrl_beta = 0.1\n",
      "Num weight bits = 24\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "creating cache_file = /data/train.txt/merged.txt.cache\n",
      "Reading datafile = /data/train.txt/merged.txt\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "\n",
      "finished run\n",
      "number of examples = 0\n",
      "weighted example sum = 0.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = n.a.\n",
      "total feature number = 0\n",
      "CPU times: user 0 ns, sys: 12 ms, total: 12 ms\n",
      "Wall time: 169 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "! LD_LIBRARY_PATH=/usr/local/lib vw -d /data/train.txt/merged.txt -b 24 -c -k --ftrl --passes 1 -f /data/model --holdout_off --loss_function logistic --random_seed 42 --progress 8000000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Check VW test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only testing\n",
      "predictions = /data/test_predictions.txt\n",
      "Num weight bits = 24\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = /data/test.txt/merged.txt\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 0\n",
      "passes used = 1\n",
      "weighted example sum = 0.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = n.a.\n",
      "total feature number = 0\n",
      "CPU times: user 12 ms, sys: 0 ns, total: 12 ms\n",
      "Wall time: 115 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "! LD_LIBRARY_PATH=/usr/local/lib vw -d /data/test.txt/merged.txt -i /data/model -t -k -p /data/test_predictions.txt --progress 1000000 --link=logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read_vw_predictions(p):\n",
    "    y_pred = []\n",
    "    with open(p, \"r\") as f:\n",
    "        for line in f:\n",
    "            y_pred.append(float(line.split()[0]))\n",
    "    return np.array(y_pred)\n",
    "\n",
    "y_pred = read_vw_predictions(\"/data/test_predictions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_vw_y_true(p):\n",
    "    y_true = []\n",
    "    with open(p, \"r\") as f:\n",
    "        for line in f:\n",
    "            y_true.append(float(line.partition(\" \")[0]))\n",
    "    return np.array(y_true)\n",
    "\n",
    "y_true = get_vw_y_true(\"/data/test.txt/merged.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-e1fe70993a93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.pyc\u001b[0m in \u001b[0;36mlog_loss\u001b[0;34m(y_true, y_pred, eps, normalize, sample_weight, labels)\u001b[0m\n\u001b[1;32m   1606\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mlogarithm\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnatural\u001b[0m \u001b[0mlogarithm\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m     \"\"\"\n\u001b[0;32m-> 1608\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1609\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    414\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m                              % (n_samples, shape_repr, ensure_min_samples,\n\u001b[0;32m--> 416\u001b[0;31m                                 context))\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "log_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "roc_auc_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Make submission to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mThe directory '/home/ubuntu/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mThe directory '/home/ubuntu/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: kaggle in /usr/local/lib/python2.7/dist-packages\n",
      "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python2.7/dist-packages (from kaggle)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python2.7/dist-packages (from kaggle)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python2.7/dist-packages (from kaggle)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python2.7/dist-packages (from kaggle)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python2.7/dist-packages (from kaggle)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests->kaggle)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests->kaggle)\n",
      "/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:318: SNIMissingWarning: An HTTPS request has been made, but the SNI (Subject Name Indication) extension to TLS is not available on this platform. This may cause the server to present an incorrect TLS certificate, which can cause validation failures. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/security.html#snimissingwarning.\n",
      "  SNIMissingWarning\n",
      "/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:122: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/security.html#insecureplatformwarning.\n",
      "  InsecurePlatformWarning\n",
      "/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:122: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/security.html#insecureplatformwarning.\n",
      "  InsecurePlatformWarning\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Unauthorized: you must download an API key from https://www.kaggle.com/<username>/account\n",
      "Then put kaggle.json in the folder /home/ubuntu/.kaggle\n"
     ]
    }
   ],
   "source": [
    "!sudo pip install kaggle\n",
    "!kaggle -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# usage: kaggle competitions submit [-h] [-c COMPETITION] -f FILE -m MESSAGE\n",
    "#                                   [-q]\n",
    "\n",
    "# required arguments:\n",
    "#   -f FILE, --file FILE  File for upload (full path)\n",
    "#   -m MESSAGE, --message MESSAGE\n",
    "#                         Message describing this submission\n",
    "\n",
    "# optional arguments:\n",
    "#   -h, --help            show this help message and exit\n",
    "#   -c COMPETITION, --competition COMPETITION\n",
    "#                         Competition URL suffix (use \"kaggle competitions list\" to show options)\n",
    "#                         If empty, the default competition will be used (use \"kaggle config set competition\")\"\n",
    "#   -q, --quiet           Suppress printing information about download progress\n",
    "!kaggle competitions submit -c outbrain-click-prediction -f ... -m \"First\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2. Better model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This time let's make a personalized recommender using:\n",
    "- page views information\n",
    "- document properties\n",
    "\n",
    "Ideas for features:\n",
    "- uuid topic, entity, publisher, ... preferences\n",
    "- document similarities\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## More SQL examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-----------+---------+--------+------------+\n",
      "|display_id|          uuid|document_id|timestamp|platform|geo_location|\n",
      "+----------+--------------+-----------+---------+--------+------------+\n",
      "|         1|cb8c55702adb93|     379743|       61|       3|   US>SC>519|\n",
      "|         2|79a85fa78311b9|    1794259|       81|       2|   US>CA>807|\n",
      "|         3|822932ce3d8757|    1179111|      182|       2|   US>MI>505|\n",
      "+----------+--------------+-----------+---------+--------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we start with a DataFrame\n",
    "events_df = ss.sql(\"select * from events\")\n",
    "events_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(display_id=u'1', uuid=u'cb8c55702adb93', document_id=u'379743', timestamp=u'61', platform=u'3', geo_location=u'US>SC>519'),\n",
       " Row(display_id=u'2', uuid=u'79a85fa78311b9', document_id=u'1794259', timestamp=u'81', platform=u'2', geo_location=u'US>CA>807'),\n",
       " Row(display_id=u'3', uuid=u'822932ce3d8757', document_id=u'1179111', timestamp=u'182', platform=u'2', geo_location=u'US>MI>505')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can make RDD of Rows with *.rdd\n",
    "from pyspark.sql import Row\n",
    "events_df.rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(bar=u'cb8c55702adb93', foo=[u'US', u'SC', u'519']),\n",
       " Row(bar=u'79a85fa78311b9', foo=[u'US', u'CA', u'807']),\n",
       " Row(bar=u'822932ce3d8757', foo=[u'US', u'MI', u'505'])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When it's RDD, we can use Python to create new RDD of Rows\n",
    "(\n",
    "    events_df.rdd\n",
    "    .map(lambda x: Row(foo=x.geo_location.split(\">\"), bar=x.uuid))\n",
    ").take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+\n",
      "|           bar|          foo|\n",
      "+--------------+-------------+\n",
      "|cb8c55702adb93|[US, SC, 519]|\n",
      "|79a85fa78311b9|[US, CA, 807]|\n",
      "|822932ce3d8757|[US, MI, 505]|\n",
      "+--------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can convert it back to DataFrame if it's still a table that can be converted to Java types\n",
    "ss.createDataFrame(\n",
    "    events_df.rdd\n",
    "    .map(lambda x: Row(foo=x.geo_location.split(\">\"), bar=x.uuid))\n",
    ").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36 ms, sys: 8 ms, total: 44 ms\n",
      "Wall time: 3min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# we can save it to HDFS as parquet (if it's a DataFrame)\n",
    "ss.createDataFrame(\n",
    "    events_df.rdd\n",
    "    .map(lambda x: Row(foo=x.geo_location.split(\">\") if x.geo_location else [], bar=x.uuid))\n",
    ").write.mode(\"overwrite\").parquet(\"/task1/example1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bar: string (nullable = true)\n",
      " |-- foo: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+--------------+-------------+\n",
      "|           bar|          foo|\n",
      "+--------------+-------------+\n",
      "|cb8c55702adb93|[US, SC, 519]|\n",
      "|79a85fa78311b9|[US, CA, 807]|\n",
      "|822932ce3d8757|[US, MI, 505]|\n",
      "+--------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss.read.parquet(\"/task1/example1\").printSchema()\n",
    "ss.read.parquet(\"/task1/example1\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36 ms, sys: 0 ns, total: 36 ms\n",
      "Wall time: 2min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# or we can skip DataFrame API if we use Python functions (there will be no speed increase)\n",
    "(\n",
    "    events_df.rdd\n",
    "    .map(lambda x: Row(foo=x.geo_location.split(\">\") if x.geo_location else [], bar=x.uuid))\n",
    ").saveAsPickleFile(\"/task1/example2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(bar=u'cb8c55702adb93', foo=[u'US', u'SC', u'519']),\n",
       " Row(bar=u'79a85fa78311b9', foo=[u'US', u'CA', u'807']),\n",
       " Row(bar=u'822932ce3d8757', foo=[u'US', u'MI', u'505'])]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.pickleFile(\"/task1/example2\").take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(x=array([u'US', u'SC', u'519'], \n",
       "      dtype='<U3')),\n",
       " Row(x=array([u'US', u'CA', u'807'], \n",
       "      dtype='<U3'))]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sometimes we cannot make a DataFrame\n",
    "import numpy as np\n",
    "rdd = (\n",
    "    events_df.rdd\n",
    "    .map(lambda x: Row(x=np.array(x.geo_location.split(\">\") if x.geo_location else [])))\n",
    ")\n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# throws TypeError: not supported type: <type 'numpy.ndarray'>\n",
    "ss.createDataFrame(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28 ms, sys: 8 ms, total: 36 ms\n",
      "Wall time: 3min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# but we can save as RDD in pickle file just fine\n",
    "rdd.saveAsPickleFile(\"/task1/example3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Takeaways:\n",
    "- use DataFrames when you can (simple join's, select's, groupby's), it will be faster\n",
    "- use RDD and Python when you can't use DataFrame API\n",
    "- convert it back to DataFrame if needed\n",
    "- or save to pickles (can save almost any Python object as pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Built-in SQL functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can find more at https://spark.apache.org/docs/2.1.0/api/java/org/apache/spark/sql/functions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|document_id|          categories|\n",
      "+-----------+--------------------+\n",
      "|     100010|[[1513,0.79842798...|\n",
      "|    1000240|[[1505,0.92], [15...|\n",
      "|    1000280|[[1909,0.92], [19...|\n",
      "+-----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sql version\n",
    "df = ss.sql(\"\"\"\n",
    "select\n",
    "    document_id,\n",
    "    collect_list(struct(category_id, confidence_level)) as categories\n",
    "from\n",
    "    documents_categories\n",
    "group by document_id\n",
    "\"\"\")\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.write.mode(\"overwrite\").parquet(\"/task1/example4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'2797448', [(u'1503', u'0.275620316'), (u'1210', u'0.020971111')]),\n",
       " (u'736183', [(u'1808', u'0.92'), (u'1513', u'0.07')]),\n",
       " (u'616955', [(u'1403', u'0.311842556'), (u'1210', u'0.023727151')])]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or we can use RDD and Python if we are not aware of those SQL functions\n",
    "rdd = (\n",
    "    ss.sql(\"select * from documents_categories\")\n",
    "    .rdd\n",
    "    .map(lambda x: (x.document_id, (x.category_id, x.confidence_level)))\n",
    "    .groupByKey()\n",
    "    .map(lambda (k, vs): (k, list(vs)))\n",
    ")\n",
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28 ms, sys: 12 ms, total: 40 ms\n",
      "Wall time: 3min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# it's much slower, but we can do almost everything in Python\n",
    "rdd.saveAsPickleFile(\"/task1/example5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# but sometimes with Python we can do more\n",
    "rdd = (\n",
    "    ss.sql(\"select * from documents_categories\")\n",
    "    .rdd\n",
    "    .map(lambda x: (x.document_id, (x.category_id, x.confidence_level)))\n",
    "    .groupByKey()\n",
    "    .map(lambda (k, vs): Row(document_id=k, categories={a: float(b) for a, b in vs}))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32 ms, sys: 4 ms, total: 36 ms\n",
      "Wall time: 25.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# much faster thanks to conversion back to DataFrame (works for simple python collections in columns)\n",
    "ss.createDataFrame(rdd).write.parquet(\"/task1/example6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|          categories|document_id|\n",
      "+--------------------+-----------+\n",
      "|Map(1510 -> 0.887...|    1059269|\n",
      "|Map(1408 -> 0.92,...|    1050604|\n",
      "|Map(1903 -> 0.92,...|    1472688|\n",
      "+--------------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss.read.parquet(\"/task1/example6\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# now we can join this table with events for instance\n",
    "ss.read.parquet(\"/task1/example6\").registerTempTable(\"doc_categories_ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-----------+---------+--------+------------+--------------------+\n",
      "|display_id|          uuid|document_id|timestamp|platform|geo_location|          categories|\n",
      "+----------+--------------+-----------+---------+--------+------------+--------------------+\n",
      "|  18242074|e703634e3dfa39|    1000240|536236046|       2|          NG|Map(1503 -> 0.07,...|\n",
      "|  18694427|5b023d28c0a9f3|    1000240|687121504|       2|   US>MA>521|Map(1503 -> 0.07,...|\n",
      "|   3436070|55e1db49ff4eef|    1000240|223783698|       1|   US>CA>803|Map(1503 -> 0.07,...|\n",
      "+----------+--------------+-----------+---------+--------+------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss.sql(\"\"\"\n",
    "select \n",
    "    e.*, \n",
    "    dc.categories\n",
    "from \n",
    "    events as e\n",
    "    join doc_categories_ready as dc on dc.document_id = e.document_id\n",
    "\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
